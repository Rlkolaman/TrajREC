{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "def parse_and_fuse_timestamp(time_str, time_nano):\n",
    "    # Parse the time string (e.g., \"2024-04-17 09:19:41\") and timeNano (e.g., \"322223\")\n",
    "    dt = datetime.strptime(f\"{time_str}_{time_nano.split('_')[-1]}\", \"%Y-%m-%d_%H_%M_%S_%f\")\n",
    "    return dt\n",
    "\n",
    "def process_pkl_files(meta_dir_root_dir, output_dir, background_image_name):\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Dictionary to store tracks, where each key is a Label and value is a list of detections\n",
    "    tracks = defaultdict(list)\n",
    "\n",
    "    # create a frame with the background image\n",
    "    background_image = cv2.imread(background_image_name)\n",
    "    # Iterate through all .pkl files in the Meta directory\n",
    "    meta_dir = os.path.join(meta_dir_root_dir, 'Meta')\n",
    "    list_of_pkl_files = os.listdir(meta_dir)\n",
    "    image_dir = os.path.join(meta_dir_root_dir, 'Images')\n",
    "    list_of_jpg_files = os.listdir(image_dir)\n",
    "    # max_x = 0\n",
    "    # max_y = 0\n",
    "    for pkl_indx, pkl_file in enumerate(list_of_pkl_files):\n",
    "        # background_image_copy = copy.deepcopy(background_image)\n",
    "        if pkl_file.endswith('.pkl'):\n",
    "            # frame_bbox_image = cv2.imread(os.path.join(image_dir, list_of_jpg_files[pkl_indx]))\n",
    "            pkl_path = os.path.join(meta_dir, pkl_file)\n",
    "            # Read the .pkl file\n",
    "            with open(pkl_path, 'rb') as f:\n",
    "                data = pkl.load(f)\n",
    "\n",
    "            # Extract required fields\n",
    "            label = data['Label']\n",
    "            time_nano = data['timeNano']\n",
    "            time = data['time']\n",
    "            box = data['BOX']\n",
    "\n",
    "            # Fuse time and timeNano for sorting\n",
    "            fused_timestamp = parse_and_fuse_timestamp(time, time_nano)\n",
    "\n",
    "            # change fused_timestamp to a counter in a resolution of 1/30 seconds from 1/1/1900\n",
    "            fused_timestamp = int((fused_timestamp - datetime(2023, 2, 9)).total_seconds() * 30)\n",
    "\n",
    "            # Convert BOX [x1, y1, x2, y2] to four points [x1, y1, x2, y1, x2, y2, x1, y2]\n",
    "            y1, x1, y2, x2 = [int(x) for x in box]\n",
    "            box_points = [x1, y1, x2, y1, x1, y2, x2, y2]\n",
    "            # if max_x < x2:\n",
    "            #     max_x = x2\n",
    "            # if max_y < y2:\n",
    "            #     max_y = y2\n",
    "            # #put the frame_bbox_image inside backjground_image at location of box_points\n",
    "            # background_image_copy[x1:x2, y1:y2] = frame_bbox_image\n",
    "            # cv2.imwrite(os.path.join(output_dir, background_image_name), background_image_copy)\n",
    "\n",
    "            # print(label)\n",
    "            # Append detection to the track for this Label\n",
    "            tracks[label].append({\n",
    "                'timestamp': fused_timestamp,\n",
    "                'box_points': box_points,\n",
    "                'source_file_name':pkl_file.split('.pkl')[0]\n",
    "                # 'background_image': background_image_copy\n",
    "            })\n",
    "            print(f'\\rprocessing pkl #{pkl_indx}/{len(list_of_pkl_files)}', end=\"\")\n",
    "\n",
    "    # print(f'max_y={max_y}')\n",
    "    # print(f'max_x = {max_x}')\n",
    "    # Process each track (Label) and save its detections to a CSV file\n",
    "    indx = 0\n",
    "    for label, detections in tracks.items():\n",
    "        indx+=1\n",
    "        print(f'\\rprocessing track #{indx}/{len(list_of_pkl_files)}', end=\"\")\n",
    "\n",
    "        # Sort detections by timestamp to ensure chronological order within the track\n",
    "        detections = sorted(detections, key=lambda x: x['timestamp'])\n",
    "\n",
    "        # Prepare data for CSV: each row is a detection in the track\n",
    "        csv_data = []\n",
    "        for i, detection in enumerate(detections, 0):  # 0-based index for detections\n",
    "            box_points = detection['box_points']\n",
    "            # Pad box points with zeros to reach 32 values\n",
    "            padded_points = box_points + [0.0] * (33 - len(box_points))\n",
    "            # Create row: detection index + box points + final zero\n",
    "            row = [detection['timestamp']] + padded_points + [0.0] + [detection['source_file_name']]\n",
    "            csv_data.append(row)\n",
    "\n",
    "        # Create a DataFrame without headers for the track\n",
    "        df = pd.DataFrame(csv_data)\n",
    "\n",
    "        # Define CSV filename (e.g., track_001.csv)\n",
    "        csv_filename = f\"{str(label).zfill(3)}.csv\"\n",
    "        csv_path = os.path.join(output_dir, csv_filename)\n",
    "\n",
    "        # Save the track's detections to CSV without headers\n",
    "        df.to_csv(csv_path, index=False, header=False)\n",
    "        print(f\"Saved track for label {label} to {csv_path} with {len(detections)} detections\")\n",
    "\n",
    "def main():\n",
    "    string_date = '2023_2_10'\n",
    "    # training_or_testing_str = 'training'\n",
    "    training_or_testing_str = 'testing'\n",
    "    # Define paths\n",
    "    meta_dir = '/home/pp/Desktop/datasets/elsec_dataset/frame/01/' + string_date + '/Pos'  # Path to Meta directory\n",
    "    output_dir = '/home/pp/Desktop/datasets/trajrec_data/elsec_data/'+ training_or_testing_str+'/trajectories/' + string_date  # Path to save CSV files\n",
    "    background_image_name = '/home/pp/Desktop/datasets/elsec_dataset/frame/01/elsec_road_meshulash_times_check.jpg'\n",
    "    # Process the .pkl files and save tracks\n",
    "    process_pkl_files(meta_dir, output_dir, background_image_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:06:16.173407Z",
     "start_time": "2025-05-20T11:06:16.165042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Read the label data\n",
    "label_df = pd.read_csv('/home/pp/Desktop/datasets/trajrec_data/elsec_data/testing/trajectories/2023_2_10/116915.csv',\n",
    "                       names=['time', 'x1', 'y1', 'x2', 'y2', 'x3', 'y3', 'x4', 'y4'],\n",
    "                       delimiter='\\t')\n",
    "\n",
    "# Read anomaly data\n",
    "anomaly_df = pd.read_csv('/home/pp/Desktop/datasets/elsec_dataset/9-10_02_2023_elsec_road_meshulash/anomaly.csv')\n",
    "\n",
    "# Extract label from index column \n",
    "anomaly_df['label'] = anomaly_df['index'].str.extract(r'_(\\d+)_')\n",
    "\n",
    "# Print the loaded dataframes\n",
    "# print(\"Label data:\")\n",
    "# # print(label_df)\n",
    "# print(\"\\nAnomaly data:\")\n",
    "# print(anomaly_df)\n"
   ],
   "id": "d35858ee5f3b378b",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:06:45.027569Z",
     "start_time": "2025-05-20T11:06:17.942933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get list of trajectory files\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "trajectory_path = '/home/pp/Desktop/datasets/trajrec_data/elsec_data/testing/trajectories/2023_2_10/'\n",
    "trajectory_files = os.listdir(trajectory_path)\n",
    "\n",
    "# Initialize empty list to store all times\n",
    "all_times = []\n",
    "\n",
    "# Read each trajectory file and collect times\n",
    "for indx, file in enumerate(trajectory_files):\n",
    "    print(f\"\\rProcessing trajectory {indx+1}/{len(trajectory_files)}\", end=\"\")\n",
    "    if file.endswith('.csv'):\n",
    "        df = pd.read_csv(os.path.join(trajectory_path, file),\n",
    "                         names=['time'] + [f'col{i}' for i in range(1, 36)],\n",
    "                         delimiter=',', index_col=False)\n",
    "\n",
    "        trajectory_id = file.split('.')[0]\n",
    "        times = df['time'].tolist()\n",
    "\n",
    "        # Check if this trajectory is in anomaly_df\n",
    "        is_anomaly = int(trajectory_id in anomaly_df['label'].values)\n",
    "\n",
    "        # Add times with their anomaly status\n",
    "        all_times.extend([(t, is_anomaly) for t in times])\n",
    "\n",
    "# Create DataFrame from collected times and check for duplicates\n",
    "result_df = pd.DataFrame(all_times, columns=['frame_time', 'is_anomaly'])\n",
    "\n",
    "# Sort values and check duplicates\n",
    "result_df = result_df.sort_values('frame_time')\n",
    "duplicates = result_df[result_df.duplicated(subset=['frame_time'], keep=False)]\n",
    "print(\"Duplicate frame times:\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates using boolean OR operation\n",
    "result_df = result_df.groupby('frame_time')['is_anomaly'].agg(lambda x: x.max() if any(x == 1) else 0).reset_index()\n",
    "\n",
    "# Display final sorted result\n",
    "print(\"\\nFinal sorted result without duplicates:\")\n",
    "print(result_df)\n"
   ],
   "id": "9ff02793436ef7b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trajectory 15273/15273Duplicate frame times:\n",
      "        frame_time  is_anomaly\n",
      "272002     2595038           0\n",
      "192488     2595038           0\n",
      "94743      2595038           0\n",
      "94744      2595039           0\n",
      "192489     2595039           0\n",
      "...            ...         ...\n",
      "354523     5178739           0\n",
      "145372     5178741           0\n",
      "354524     5178741           0\n",
      "145373     5178742           0\n",
      "354525     5178742           0\n",
      "\n",
      "[149055 rows x 2 columns]\n",
      "\n",
      "Final sorted result without duplicates:\n",
      "        frame_time  is_anomaly\n",
      "0          2592840           0\n",
      "1          2592858           0\n",
      "2          2592859           0\n",
      "3          2592861           0\n",
      "4          2592862           0\n",
      "...            ...         ...\n",
      "358340     5183981           0\n",
      "358341     5183983           0\n",
      "358342     5183985           0\n",
      "358343     5183988           0\n",
      "358344     5183990           0\n",
      "\n",
      "[358345 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:08:29.156357Z",
     "start_time": "2025-05-20T11:08:29.047495Z"
    }
   },
   "cell_type": "code",
   "source": "result_df.to_csv('/home/pp/Desktop/datasets/trajrec_data/elsec_data/testing/trajectories/2023_2_10/result.csv', index=False)",
   "id": "120df12402b6b0ce",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "bfe297f1694054a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
